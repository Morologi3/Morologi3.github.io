---
title: 'Attention is All you need Key ideas'
date: 2024-08-03
permalink: /posts/2012/08/blog-post-1/
tags:
  - paperExplained
  - transformers
  - keyIdeas
---

In this post I will include key ideas from the transformer model paper "Attention is All You Need" by Vaswani, A et al,.

Paper Key Ideas
======
1. Introducing transformers that replaces Recurrent Neural Networks (RNNs) and solve the issue of sequences
2. Self-Attention Mechanism which let us to take the input sequence at once, in the contrary of RNNs which divide the text into sequences
3. Multi-Head Attention which capture various relationships within data
4. Transformers uses Encoder-Decoder structure which makes a new trend in AI tech
5. Performance on translation tasks which was incredible
6. It changed the future research directions which makes it a turning point
7. Transformers can be applied beyond text
8. Finally, efficiency in training which was faster

Why transformers are efficient?
======
Due to solving the sequences issue in RNNs, self-Attention and Multi-Head attention, the encoder-decoder structure, and faster training, all makes this paper a turning point in AI world.

